# ID Block

- @Author Eric Zair
- @Project Reddit Analysis

## Data and Features

As a reminder the data we are analyzing is in a mongodb database with each record in the form:

```python
submission_comment_record = {
    'author': author_id,
    'body': submission_comment.body,
    'created_at': submission_comment.created_utc,
    'distinguished': submission_comment.distinguished,
    'edited': submission_comment.edited,
    '_id': submission_comment.id,
    'is_submitter': submission_comment.is_submitter,
    'link_id': submission_comment.link_id,
    'parent_id': submission_comment.parent_id,
    'replies': replies,
    'score': submission_comment.score,
    'stickied': submission_comment.stickied,
    'submission': submission_comment.submission.id,
    'subreddit_name': submission_comment.subreddit.display_name,
    'subreddit_id': submission_comment.subreddit_id,
    'sorting_type': sorting_type
}
```

The feature here used is the `body` field, which contains the actual text or message of a comment. E.g. "Eric went to the store." could be an example of a comment.

## Preprocessing Data

After all data is scrapped using the `reddit_post_collector.py` script, we immediately preprocess the data. To do this I have created my own preprocessing method for a comment that:

1. Tokenizes a entire string into a list of words.
2. Removes all punctuation from a token.
3. Takes each token and converts them to lower case.
4. Removes any tokens that are in our list of stop words (converted to lowercase)
5. Stem each word into it's original form. E.g. "Running" becomes "run".
6. Then recombine each string of tokens back into one string.

### Benefits of preprocessing

This processes might show the body of a comment to be a jumbled up string of some sort, however, it appears that my analysis results showed stronger when removing these.

My logic is that the `SentimentIntensityAnalyzer` I used (which is contained in the vaderSentiment library) accounts for things like punctuation and will evaluate symbols like `!!!` stronger than that of `!`. I do not want those symbols to affect my results, since I really care about how "toxic" something is. The vulgarity of a subreddit is more important of a rating to be than, the level of intensity generated by a `!` symbol.

The `SentimentIntensityAnalyzer` also buts more emphasis on things that are capitalized, so `VADER` would be more important than `vader` for example. My analysis however, puts everything to lowercase because again, I care about toxicity level and things like vulgarity rather than the casing of the bad word that is used. I want them to be evaluated at the same level.

## Analyzing Subsets of Data

Since analyzing `N` amount of posts each with `K` amount of comments in it takes an extremely long amount of time, I have added the feature for the user to set a max amount of comments and a max amount of subreddit to analyze. This is an extremely nice feature because it saves a huge amount of time.

Analyzing on subsets of data does come with its weaknesses. The results we receive tend to not be as accurate as the ones where we analyze the entire set of data that we have. The more results we analyze, the less significant each individual.

Let's take a look of an example of running our results on a subreddit called `battlestations`. This subreddit contains a lot of comments on really nice looking computer builds, so we are expecting our results to be more positive than negative.

### Analyzing `battlestations` with 10 submissions to analyze

*The call made in our program*:

```python
def test_subreddit_call(analyzer):
    hmft_anylsis_results = analyzer.analyze_subreddit('battlestations',
                                                      display_all_submission_results=True,
                                                      display_all_comment_results=False,
                                                      max_number_of_submissions_to_analyze=10)

```

```txt
Subreddit: battlestations:
Submission_id: gb94j5:
Positivity Rating: 0.8816958880983965
Negativity Rating: 0.11830411190160393

Subreddit: battlestations:
Submission_id: gbgukq:
Positivity Rating: 1.8168480845702057
Negativity Rating: 0.18315191542979486

Subreddit: battlestations:
Submission_id: gbhpz4:
Positivity Rating: 2.762072779645218
Negativity Rating: 0.23792722035478248

Subreddit: battlestations:
Submission_id: gbiveg:
Positivity Rating: 3.7508241857194586
Negativity Rating: 0.24917581428054175

Subreddit: battlestations:
Submission_id: gbk8m4:
Positivity Rating: 4.596907174577039
Negativity Rating: 0.40309282542296165

Subreddit: battlestations:
Submission_id: gbl717:
Positivity Rating: 5.538387929351715
Negativity Rating: 0.4616120706482856

Subreddit: battlestations:
Submission_id: gbnovg:
Positivity Rating: 6.4527414626089525
Negativity Rating: 0.5472585373910479

Subreddit: battlestations:
Submission_id: gbsb5k:
Positivity Rating: 7.2358995257062375
Negativity Rating: 0.7641004742937629

Subreddit: battlestations:
Submission_id: gbt4p4:
Positivity Rating: 8.16264699677158
Negativity Rating: 0.8373530032284208

Subreddit: battlestations:
Submission_id: gbt7gc:
Positivity Rating: 8.98849506496717
Negativity Rating: 1.0115049350328298


Results of all comments for : "battlestations"
Average positivity: 89.884951%
Average negativity: 10.115049%
Total time: 0:00:02.122480
```

### Analyzing `battlestations` with all 150 submissions in our database

*The call made in our program*:

```python
def test_subreddit_call(analyzer):
    hmft_anylsis_results = analyzer.analyze_subreddit('battlestations',
                                                      display_all_submission_results=True,
                                                      display_all_comment_results=False)

```

```txt
Results of all comments for : "battlestations"
Average positivity: 88.890652%
Average negativity: 11.109348%
Total time: 0:00:31.545274
```

**Note**: There were too many results to display to you.

So, in this particular case we see that after analyzing on 150 posts, the results for this subreddit happen to be extremely close to the same. Note that this subreddit tends to be positive in general, based off of the type of content that we see here.

Now, if we are to analyze a more dark subreddit such as `holdmyfeedingtube` we will see that the results for this one will a bit different. Part of this has to do with some users posting bad toxic content and other users calling them out stating that the content is bad.

### Analyzing `holdmyfeedingtube` with 10 submissions in our database

*The call made in our program*:

```python

def test_subreddit_call(analyzer):
    hmft_anylsis_results = analyzer.analyze_subreddit('holdmyfeedingtube',
                                                      display_all_submission_results=True,
                                                      display_all_comment_results=False,
                                                      max_number_of_submissions_to_analyze=10)
```

```txt
Subreddit: holdmyfeedingtube:
Submission_id: bbpozw:
Positivity Rating: 0.19428229423332527
Negativity Rating: 0.8057177057666747

Subreddit: holdmyfeedingtube:
Submission_id: bkx7tp:
Positivity Rating: 0.7209346077070591
Negativity Rating: 1.2790653922929411

Subreddit: holdmyfeedingtube:
Submission_id: c6jb15:
Positivity Rating: 1.10393082640027
Negativity Rating: 1.8960691735997286

Subreddit: holdmyfeedingtube:
Submission_id: ca8q81:
Positivity Rating: 1.4531490665327262
Negativity Rating: 2.546850933467273

Subreddit: holdmyfeedingtube:
Submission_id: cgm7wj:
Positivity Rating: 2.0233354395757486
Negativity Rating: 2.9766645604242505

Subreddit: holdmyfeedingtube:
Submission_id: cmmby0:
Positivity Rating: 2.392031991629216
Negativity Rating: 3.6079680083707837

Subreddit: holdmyfeedingtube:
Submission_id: cxqkhc:
Positivity Rating: 2.822155341211233
Negativity Rating: 4.177844658788767

Subreddit: holdmyfeedingtube:
Submission_id: d3596a:
Positivity Rating: 3.2467348905132534
Negativity Rating: 4.753265109486747

Subreddit: holdmyfeedingtube:
Submission_id: ddd58l:
Positivity Rating: 3.7966311230928684
Negativity Rating: 5.203368876907132

Subreddit: holdmyfeedingtube:
Submission_id: dh306y:
Positivity Rating: 4.214073286417404
Negativity Rating: 5.7859267135825965


Results of all comments for : "holdmyfeedingtube"
Average positivity: 42.140733%
Average negativity: 57.859267%
Total time: 0:00:03.655826
```

*The call made in our program*:

```python
def test_subreddit_call(analyzer):
    hmft_anylsis_results = analyzer.analyze_subreddit('holdmyfeedingtube',
                                                      display_all_submission_results=True,
                                                      display_all_comment_results=False)
```

### Analyzing `holdmyfeedingtube` with all 150 submissions in our database

```txt
Results of all comments for : "holdmyfeedingtube"
Average positivity: 48.362596%
Average negativity: 51.637404%
Total time: 0:00:39.465240
```

The difference of negativity between the subset analyed and the entire data set is a solid 6.2%, which may no sound like a huge amount, but it does make a difference.

## Analyzing a subset of comments

We also have the ability to limit the amount of comments that is read from each individual post. This can largely change the results that we receive from the program's analysis as well.

I will now show an example of running the program with analyzing 10 comments per each submission, v.s. analyzing every single comment in each submission, which is what we have been doing above.

### Analyzing holdmyfeedingtube with 1 comment from each submission

*The call made in our program*:

```python
def test_subreddit_call(analyzer):
    hmft_anylsis_results = analyzer.analyze_subreddit('holdmyfeedingtube',
                                                      display_all_submission_results=True,
                                                      display_all_comment_results=False,
                                                      max_number_of_comments_to_analyze=1)
```

```txt
Results of all comments for: "holdmyfeedingtube"
Average positivity: 50.400000%
Average negativity: 49.600000%
Total time: 0:00:12.114495
```

### Analyzing holdmyfeedingtube with all comments from each submission

*The call made in our program*:

```python
def test_subreddit_call(analyzer):
    hmft_anylsis_results = analyzer.analyze_subreddit('holdmyfeedingtube',
                                                      display_all_submission_results=True,
                                                      display_all_comment_results=False)
```

```txt
Results of all comments for : "holdmyfeedingtube"
Average positivity: 48.362596%
Average negativity: 51.637404%
Total time: 0:00:39.171607
```

As you can see, there is about a 2% difference between these results.

### Analyzing holdmyfeedingtube wth 1 comment from 10 submissions

Now if we were to subset the amount of submissions to lets say 10, and then limit the number of comments to each posts to 1, then the results would be a even more distinct and further apart.

*The call made in our program*:

```python
def test_subreddit_call(analyzer):
    hmft_anylsis_results = analyzer.analyze_subreddit('holdmyfeedingtube',
                                                      display_all_submission_results=True,
                                                      display_all_comment_results=False,
                                                      max_number_of_comments_to_analyze=1,
                                                      max_number_of_submissions_to_analyze=10)
```

```txt
Results of all comments for : "holdmyfeedingtube"
Average positivity: 42.857143%
Average negativity: 57.142857%
Total time: 0:00:00.810183
```

The main thing to note here is that depending on what kind of data you want, you need to choose the proper range.

If you want the absolute best results, then you want to ensure that you analyze all submissions and comments in the database.

If you want to get a quicker results but still get semi-accurate results, it is best to analyze about 100 submissions with 100 comments each, ensuring you have a data set of 10,000 to analyze.

## Analyzing Posts By Sorting Type

You can also analyze posts by a given sorting type.

In other words, if you want to find the hottest posts at the current moment (based off the data in your database), then you can do so.

If you you want analyze only the newest posts (in your database), then you can do so.

If you want to analyze the top or most liked posts (in your database) then you can do that as well.

The flags are `new`, `top`, and `hot`. These can be given to the `analyze_subreddit()`.

Let's take a look at a call analyzing posts from `battlestations` with the `hot` flag so that we can analyze our the subreddit based off of reviews of nice computer gaming setups.

### Analyzing all  hot posts on battlestations

*The call made in our program*:

```python
hmft_anylsis_results = analyzer.analyze_subreddit('holdmyfeedingtube',
                                                      display_all_submission_results=True,
                                                      display_all_comment_results=False,
                                                      sorting_type='new')
```

```txt
Results of all comments for : "battlestations"
Average positivity: 88.890652%
Average negativity: 11.109348%
Total time: 0:00:26.404022
```

Hmm, seems like people really enjoyed some of these gaming setups!
